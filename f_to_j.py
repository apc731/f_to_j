import os
import opencc
import zipfile
import shutil
import chardet
import logging
import tempfile
import re
from bs4 import BeautifulSoup
from bs4.element import NavigableString
from datetime import datetime

# é…ç½®æ—¥å¿—
log_file = f"epub_convert_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename=log_file,
    filemode='w'
)

def convert_epub(epub_file_path):
    """
    å°†EPUBæ–‡ä»¶ä»ç¹ä½“è½¬ä¸ºç®€ä½“
    """
    logging.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {epub_file_path}")
    
    # åˆ›å»ºæºæ–‡ä»¶å¤‡ä»½
    backup_file = epub_file_path.replace('.epub', '_backup.epub')
    if not os.path.exists(backup_file):
        shutil.copy2(epub_file_path, backup_file)
        print(f"âœ… å·²åˆ›å»ºæºæ–‡ä»¶å¤‡ä»½: {backup_file}")
        logging.info(f"å·²åˆ›å»ºæºæ–‡ä»¶å¤‡ä»½: {backup_file}")
    
    # åˆå§‹åŒ– OpenCCï¼ˆç¹ä½“è½¬ç®€ä½“ï¼‰- ä¿®å¤é…ç½®æ–‡ä»¶è·¯å¾„é—®é¢˜
    try:
        # å°è¯•ç›´æ¥ä½¿ç”¨é…ç½®åç§°
        converter = opencc.OpenCC('t2s')
    except Exception as e:
        logging.warning(f"ä½¿ç”¨'t2s'åˆå§‹åŒ–OpenCCå¤±è´¥: {e}")
        try:
            # å°è¯•ä½¿ç”¨å®Œæ•´è·¯å¾„
            opencc_path = os.path.dirname(opencc.__file__)
            config_path = os.path.join(opencc_path, 'config', 't2s.json')
            if os.path.exists(config_path):
                converter = opencc.OpenCC(config_path)
            else:
                # å°è¯•ä½¿ç”¨å†…ç½®è½¬æ¢å™¨
                converter = opencc.OpenCC('t2s.json')
        except Exception as e:
            logging.error(f"åˆå§‹åŒ–OpenCCå¤±è´¥: {e}")
            print(f"âŒ åˆå§‹åŒ–ç¹ç®€è½¬æ¢å™¨å¤±è´¥: {e}")
            return False
    
    # ä½¿ç”¨ä¸´æ—¶ç›®å½•è€Œä¸æ˜¯å›ºå®šç›®å½•
    with tempfile.TemporaryDirectory() as extract_dir:
        logging.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {extract_dir}")
        
        # 1. è§£å‹ epub åˆ°ä¸´æ—¶ç›®å½•
        try:
            with zipfile.ZipFile(epub_file_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
                logging.info("æˆåŠŸè§£å‹EPUBæ–‡ä»¶")
        except Exception as e:
            logging.error(f"è§£å‹EPUBæ–‡ä»¶å¤±è´¥: {e}")
            print(f"âŒ è§£å‹EPUBæ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        # æ£€æŸ¥EPUBç»“æ„
        if not validate_epub_structure(extract_dir):
            logging.warning("EPUBç»“æ„éªŒè¯å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•è½¬æ¢")
            print("âš ï¸ EPUBç»“æ„éªŒè¯å¤±è´¥ï¼Œä½†å°†ç»§ç»­å°è¯•è½¬æ¢")
        
        # 2. è½¬æ¢æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶ä¸ºç®€ä½“ï¼ˆæä¿å®ˆæ–¹å¼ï¼‰
        file_count = 0
        changed_count = 0
        
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if file.endswith(('.xhtml', '.html', '.htm', '.xml', '.opf', '.ncx')):
                    file_path = os.path.join(root, file)
                    try:
                        # æ£€æµ‹åŸå§‹ç¼–ç 
                        with open(file_path, 'rb') as f:
                            raw = f.read()
                        
                        # ä¿å­˜åŸå§‹æ–‡ä»¶å±æ€§å’Œå†…å®¹ï¼ˆç”¨äºæ¢å¤ï¼‰
                        original_stat = os.stat(file_path)
                        original_content = raw
                        
                        detect = chardet.detect(raw)
                        encoding = detect['encoding'] if detect['encoding'] and detect['confidence'] > 0.7 else 'utf-8'
                        logging.info(f"æ–‡ä»¶ {file} æ£€æµ‹åˆ°ç¼–ç : {encoding}, ç½®ä¿¡åº¦: {detect['confidence'] if 'confidence' in detect else 'unknown'}")
                        
                        # å°è¯•è§£ç 
                        try:
                            content = raw.decode(encoding, errors='ignore')
                        except Exception as e:
                            logging.warning(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç  {encoding} è§£ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ utf-8: {e}")
                            content = raw.decode('utf-8', errors='ignore')
                        
                        file_count += 1
                        
                        # æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶å°è¯•è½¬æ¢
                        try:
                            if file.endswith(('.xhtml', '.html', '.htm')):
                                # HTMLç±»æ–‡ä»¶ä½¿ç”¨BeautifulSoupå¤„ç†
                                changed = convert_html_content(content, file_path, converter)
                            elif file.endswith(('.xml', '.opf', '.ncx')):
                                # XMLç±»æ–‡ä»¶ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç†ï¼Œæ›´å®‰å…¨
                                changed = convert_xml_content(content, file_path, converter)
                            else:
                                changed = False
                            
                            if changed:
                                changed_count += 1
                                # æ¢å¤åŸå§‹æ–‡ä»¶å±æ€§
                                os.utime(file_path, (original_stat.st_atime, original_stat.st_mtime))
                        except Exception as e:
                            # è½¬æ¢å¤±è´¥ï¼Œæ¢å¤åŸå§‹å†…å®¹
                            logging.error(f"è½¬æ¢æ–‡ä»¶ {file_path} å¤±è´¥ï¼Œæ¢å¤åŸå§‹å†…å®¹: {e}")
                            with open(file_path, 'wb') as f:
                                f.write(original_content)
                    
                    except Exception as e:
                        logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                        print(f"âš ï¸ è·³è¿‡æ— æ³•å¤„ç†çš„æ–‡ä»¶ {file_path}ï¼ŒåŸå› ï¼š{e}")
        
        logging.info(f"å…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­ {changed_count} ä¸ªæ–‡ä»¶æœ‰å˜åŒ–")
        print(f"ğŸ“Š å…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­ {changed_count} ä¸ªæ–‡ä»¶æœ‰å˜åŒ–")
        
        # 3. é‡æ–°æ‰“åŒ…ä¸ºæ–°çš„ epub æ–‡ä»¶ï¼Œç¡®ä¿ç¬¦åˆepubæ ‡å‡†
        simplified_epub = epub_file_path.replace('.epub', '_simplified.epub')
        
        # ç¡®ä¿mimetypeæ–‡ä»¶æ­£ç¡®
        ensure_mimetype_file(extract_dir)
        
        # ä½¿ç”¨æ›´å®‰å…¨çš„æ‰“åŒ…æ–¹æ³•
        if not package_epub_safely(extract_dir, simplified_epub):
            logging.error("æ‰“åŒ…EPUBæ–‡ä»¶å¤±è´¥")
            print("âŒ æ‰“åŒ…EPUBæ–‡ä»¶å¤±è´¥")
            return False
        
        # 4. éªŒè¯ç»“æœ
        if os.path.exists(simplified_epub) and os.path.getsize(simplified_epub) > 0:
            logging.info(f"æˆåŠŸç”Ÿæˆç®€ä½“EPUBæ–‡ä»¶: {simplified_epub}")
            print(f"âœ… æ–°çš„ç®€ä½“EPUBæ–‡ä»¶å·²ç”Ÿæˆ: {simplified_epub}")
            return True
        else:
            logging.error("ç”Ÿæˆç®€ä½“EPUBæ–‡ä»¶å¤±è´¥")
            print("âŒ è½¬æ¢å¤±è´¥ï¼Œæºæ–‡ä»¶ä¿æŒåŸæ ·")
            print(f"âš ï¸ è¯·æ£€æŸ¥å¤‡ä»½æ–‡ä»¶: {backup_file}")
            return False

def validate_epub_structure(extract_dir):
    """
    éªŒè¯EPUBæ–‡ä»¶ç»“æ„
    """
    # æ£€æŸ¥mimetypeæ–‡ä»¶
    mimetype_path = os.path.join(extract_dir, 'mimetype')
    if not os.path.exists(mimetype_path):
        logging.warning("ç¼ºå°‘mimetypeæ–‡ä»¶ï¼Œå°†åˆ›å»º")
        return True  # æˆ‘ä»¬ä¼šåœ¨åé¢åˆ›å»º
    
    # æ£€æŸ¥META-INFç›®å½•
    meta_inf_dir = os.path.join(extract_dir, 'META-INF')
    if not os.path.exists(meta_inf_dir):
        logging.error("ç¼ºå°‘META-INFç›®å½•")
        os.makedirs(meta_inf_dir)
        
    # æ£€æŸ¥META-INF/container.xml
    container_xml = os.path.join(extract_dir, 'META-INF', 'container.xml')
    if not os.path.exists(container_xml):
        logging.error("ç¼ºå°‘å…³é”®æ–‡ä»¶ META-INF/container.xml")
        # å°è¯•æŸ¥æ‰¾OPFæ–‡ä»¶å¹¶åˆ›å»ºcontainer.xml
        opf_files = []
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if file.endswith('.opf'):
                    rel_path = os.path.relpath(os.path.join(root, file), extract_dir)
                    rel_path = rel_path.replace(os.sep, '/')
                    opf_files.append(rel_path)
        
        if opf_files:
            create_container_xml(container_xml, opf_files[0])
            logging.info(f"å·²åˆ›å»ºcontainer.xmlï¼ŒæŒ‡å‘: {opf_files[0]}")
            return True
        return False
    
    # æ£€æŸ¥OPFæ–‡ä»¶
    try:
        with open(container_xml, 'r', encoding='utf-8', errors='ignore') as f:
            container_content = f.read()
        
        # æŸ¥æ‰¾OPFæ–‡ä»¶è·¯å¾„
        match = re.search(r'full-path="([^"]+)"', container_content)
        if match:
            opf_path = match.group(1)
            opf_full_path = os.path.join(extract_dir, *opf_path.split('/'))
            if not os.path.exists(opf_full_path):
                logging.error(f"OPFæ–‡ä»¶ä¸å­˜åœ¨: {opf_path}")
                return False
            logging.info(f"æ‰¾åˆ°OPFæ–‡ä»¶: {opf_path}")
        else:
            logging.error("æ— æ³•åœ¨container.xmlä¸­æ‰¾åˆ°OPFæ–‡ä»¶è·¯å¾„")
            return False
    except Exception as e:
        logging.error(f"æ£€æŸ¥OPFæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False
    
    return True

def create_container_xml(container_path, opf_path):
    """
    åˆ›å»ºcontainer.xmlæ–‡ä»¶
    """
    content = f'''<?xml version="1.0" encoding="UTF-8"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
    <rootfiles>
        <rootfile full-path="{opf_path}" media-type="application/oebps-package+xml"/>
    </rootfiles>
</container>'''
    
    os.makedirs(os.path.dirname(container_path), exist_ok=True)
    with open(container_path, 'w', encoding='utf-8') as f:
        f.write(content)

def convert_html_content(content, file_path, converter):
    """
    è½¬æ¢HTMLå†…å®¹ä¸­çš„æ–‡æœ¬
    """
    # ä¿å­˜åŸå§‹XMLå£°æ˜å’ŒDOCTYPE
    xml_declaration = None
    match = re.search(r'<\?xml[^>]+\?>', content)
    if match:
        xml_declaration = match.group(0)
    
    doctype = None
    match = re.search(r'<!DOCTYPE[^>]+>', content)
    if match:
        doctype = match.group(0)
    
    # å°è¯•ä¸åŒçš„è§£æå™¨
    parsers = ['html.parser', 'lxml', 'html5lib']
    soup = None
    
    for parser in parsers:
        try:
            soup = BeautifulSoup(content, parser)
            break
        except Exception as e:
            logging.warning(f"ä½¿ç”¨è§£æå™¨ {parser} è§£æ {file_path} å¤±è´¥: {e}")
    
    if soup is None:
        logging.error(f"æ‰€æœ‰è§£æå™¨éƒ½æ— æ³•è§£æ {file_path}")
        return False
    
    changed = False
    
    # åªæ›¿æ¢æ–‡æœ¬èŠ‚ç‚¹
    for elem in soup.find_all(string=True):
        if isinstance(elem, NavigableString):
            parent = elem.parent
            # è·³è¿‡script/styleç­‰
            if parent and parent.name in ['script', 'style']:
                continue
            
            # è½¬æ¢æ–‡æœ¬
            original_text = str(elem)
            new_text = converter.convert(original_text)
            
            if new_text != original_text:
                elem.replace_with(new_text)
                changed = True
    
    # åªåœ¨æœ‰å˜åŒ–æ—¶å†™å›
    if changed:
        # è·å–åŸå§‹æ–‡ä»¶ç¼–ç 
        with open(file_path, 'rb') as f:
            raw = f.read()
        detect = chardet.detect(raw)
        encoding = detect['encoding'] if detect['encoding'] and detect['confidence'] > 0.7 else 'utf-8'
        
        # å†™å›æ–‡ä»¶
        with open(file_path, 'w', encoding=encoding, newline='', errors='ignore') as f:
            output = str(soup)
            
            # æ¢å¤XMLå£°æ˜å’ŒDOCTYPE
            if xml_declaration and not output.startswith('<?xml'):
                output = xml_declaration + '\n' + output
            if doctype and '<!DOCTYPE' not in output:
                if xml_declaration and output.startswith('<?xml'):
                    # åœ¨XMLå£°æ˜åæ’å…¥DOCTYPE
                    output = output.replace('?>\n', '?>\n' + doctype + '\n')
                else:
                    output = doctype + '\n' + output
            
            f.write(output)
        
        logging.info(f"å·²è½¬æ¢æ–‡ä»¶: {file_path}")
    
    return changed

def convert_xml_content(content, file_path, converter):
    """
    å®‰å…¨åœ°è½¬æ¢XMLå†…å®¹ä¸­çš„æ–‡æœ¬
    """
    # ä¿å­˜åŸå§‹XMLå£°æ˜
    xml_declaration = None
    match = re.search(r'<\?xml[^>]+\?>', content)
    if match:
        xml_declaration = match.group(0)
    
    # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•å¤„ç†XML
    # åªæ›¿æ¢çº¯æ–‡æœ¬å†…å®¹ï¼Œä¸ä¿®æ”¹å±æ€§å€¼
    changed_content = content
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å’Œè½¬æ¢æ–‡æœ¬ï¼Œæ›´å®‰å…¨
    def replace_text(match):
        text = match.group(1)
        # è·³è¿‡çº¯æ•°å­—ã€ç©ºç™½ç­‰å†…å®¹
        if re.match(r'^[\s\d.,:;!?]+$', text):
            return match.group(0)
        new_text = converter.convert(text)
        if new_text != text:
            return f">{new_text}<"
        return match.group(0)
    
    # åªæ›¿æ¢æ ‡ç­¾ä¹‹é—´çš„æ–‡æœ¬
    pattern = r'>([^<>]+)<'
    new_content = re.sub(pattern, replace_text, changed_content)
    
    changed = new_content != content
    
    if changed:
        # è·å–åŸå§‹æ–‡ä»¶ç¼–ç 
        with open(file_path, 'rb') as f:
            raw = f.read()
        detect = chardet.detect(raw)
        encoding = detect['encoding'] if detect['encoding'] and detect['confidence'] > 0.7 else 'utf-8'
        
        # å†™å›æ–‡ä»¶
        with open(file_path, 'w', encoding=encoding, newline='', errors='ignore') as f:
            # å¦‚æœæœ‰XMLå£°æ˜ä¸”ä¸åœ¨è¾“å‡ºä¸­ï¼Œæ·»åŠ å›å»
            if xml_declaration and not new_content.startswith('<?xml'):
                new_content = xml_declaration + '\n' + new_content
            f.write(new_content)
        
        logging.info(f"å·²è½¬æ¢XMLæ–‡ä»¶: {file_path}")
    
    return changed

def ensure_mimetype_file(extract_dir):
    """
    ç¡®ä¿mimetypeæ–‡ä»¶æ­£ç¡®
    """
    mimetype_path = os.path.join(extract_dir, 'mimetype')
    
    # åˆ›å»ºæˆ–æ›´æ–°mimetypeæ–‡ä»¶
    with open(mimetype_path, 'wb') as f:
        f.write(b'application/epub+zip')
    
    logging.info("å·²ç¡®ä¿mimetypeæ–‡ä»¶æ­£ç¡®")

def package_epub_safely(extract_dir, output_path):
    """
    æ›´å®‰å…¨åœ°æ‰“åŒ…EPUBæ–‡ä»¶
    """
    try:
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•ï¼Œç”¨äºå­˜æ”¾è¦æ‰“åŒ…çš„æ–‡ä»¶
        with tempfile.TemporaryDirectory() as temp_dir:
            # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            for root, dirs, files in os.walk(extract_dir):
                for file in files:
                    if file in ['.DS_Store', 'Thumbs.db']:
                        continue
                    
                    src_path = os.path.join(root, file)
                    rel_path = os.path.relpath(src_path, extract_dir)
                    dst_path = os.path.join(temp_dir, rel_path)
                    
                    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                    shutil.copy2(src_path, dst_path)
            
            # ç¡®ä¿mimetypeæ–‡ä»¶æ­£ç¡®
            mimetype_path = os.path.join(temp_dir, 'mimetype')
            with open(mimetype_path, 'wb') as f:
                f.write(b'application/epub+zip')
            
            # ä½¿ç”¨å¤–éƒ¨å·¥å…·æ‰“åŒ…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if shutil.which('zip'):
                # ä½¿ç”¨zipå‘½ä»¤è¡Œå·¥å…·ï¼Œç¡®ä¿mimetypeä¸è¢«å‹ç¼©ä¸”åœ¨ç¬¬ä¸€ä½
                current_dir = os.getcwd()
                try:
                    os.chdir(temp_dir)
                    os.system(f'zip -X0 "{output_path}" mimetype')
                    os.system(f'zip -Xr9D "{output_path}" * -x mimetype')
                    os.chdir(current_dir)
                    return os.path.exists(output_path)
                except Exception as e:
                    logging.error(f"ä½¿ç”¨zipå‘½ä»¤æ‰“åŒ…å¤±è´¥: {e}")
                    os.chdir(current_dir)
            
            # å¦‚æœå¤–éƒ¨å·¥å…·ä¸å¯ç”¨ï¼Œä½¿ç”¨Pythonçš„zipfile
            mimetype_path = os.path.join(temp_dir, 'mimetype')
            
            with zipfile.ZipFile(output_path, 'w') as zipf:
                # mimetypeå¿…é¡»ç¬¬ä¸€ä¸ªä¸”æœªå‹ç¼©
                zipf.write(mimetype_path, 'mimetype', compress_type=zipfile.ZIP_STORED)
                
                # æ·»åŠ å…¶ä»–æ–‡ä»¶
                for foldername, subfolders, filenames in os.walk(temp_dir):
                    for filename in filenames:
                        if filename in ['.DS_Store', 'Thumbs.db', 'mimetype']:
                            continue
                        
                        abs_path = os.path.join(foldername, filename)
                        rel_path = os.path.relpath(abs_path, temp_dir)
                        
                        # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦
                        rel_path = rel_path.replace(os.sep, '/')
                        
                        # ç‰¹æ®Šæ–‡ä»¶ä¸å‹ç¼©
                        if rel_path == 'META-INF/container.xml':
                            zipf.write(abs_path, rel_path, compress_type=zipfile.ZIP_STORED)
                        else:
                            zipf.write(abs_path, rel_path, compress_type=zipfile.ZIP_DEFLATED)
            
            logging.info(f"æˆåŠŸæ‰“åŒ…EPUBæ–‡ä»¶: {output_path}")
            return True
    except Exception as e:
        logging.error(f"æ‰“åŒ…EPUBæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ“š EPUBç¹ä½“è½¬ç®€ä½“è½¬æ¢å·¥å…· ğŸ“š")
    print("----------------------------")
    
    # å¯ä»¥æ”¹ä¸ºä»å‘½ä»¤è¡Œå‚æ•°è·å–
    epub_file = r"C:\Users\86184\Downloads\æ¯›æ¾¤æ±ä¹‹å¾Œçš„ä¸­åœ‹ï¼šä¸€å€‹å¼·åœ‹å´›èµ·çš„çœŸç›¸ = China After Mao The Rise of a Superpower (é¦®å®¢ (Frank DikÃ¶tter) è‘—, è•­è‘‰ è­¯) (Z-Library).epub"
    
    # ä¹Ÿå¯ä»¥è®©ç”¨æˆ·è¾“å…¥æ–‡ä»¶è·¯å¾„
    # epub_file = input("è¯·è¾“å…¥EPUBæ–‡ä»¶è·¯å¾„: ").strip('"')
    
    if not os.path.exists(epub_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {epub_file}")
        return
    
    print(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶: {epub_file}")
    success = convert_epub(epub_file)
    
    if success:
        print("âœ… è½¬æ¢å®Œæˆï¼")
    else:
        print("âŒ è½¬æ¢å¤±è´¥ï¼")
    
    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜è‡³: {log_file}")

if __name__ == "__main__":
    main()
